# üß™ MVP ‚Äì Engenharia de Dados: An√°lise da COVID-19

## üìå Descri√ß√£o

Neste trabalho, foi constru√≠do um pipeline de dados utilizando tecnologias na nuvem (Databricks). O pipeline envolveu as etapas de **busca, coleta, modelagem, carga e an√°lise** de dados p√∫blicos sobre **casos, mortes e vacina√ß√£o** contra a COVID-19 ao redor do mundo. O projeto foi realizado como parte da disciplina de Engenharia de Dados, com foco em pr√°ticas de modelagem dimensional e uso de ferramentas de Big Data.

---

## üéØ Objetivo

### Objetivo Geral

Desenvolver um MVP de engenharia de dados na nuvem, utilizando o **Databricks**, para consolidar e analisar dados sobre a pandemia de **COVID-19**, com foco em compreender os impactos da vacina√ß√£o na evolu√ß√£o de casos e mortes, atrav√©s de um modelo de dados anal√≠tico em **estrela**.

### Objetivos Espec√≠ficos

1. Realizar a ingest√£o, tratamento e transforma√ß√£o dos dados de casos/mortes e vacina√ß√£o no Databricks.  
2. Modelar os dados utilizando a abordagem dimensional (modelo estrela), com fato e dimens√µes adequadas.  
3. Integrar as duas bases de dados em um modelo anal√≠tico √∫nico, utilizando chaves comuns como pa√≠s e data.  
4. Construir visualiza√ß√µes e an√°lises explorat√≥rias para responder perguntas relevantes sobre a pandemia.  
5. Identificar padr√µes e poss√≠veis correla√ß√µes entre o avan√ßo da vacina√ß√£o e a redu√ß√£o de casos/mortes.  
6. Avaliar a qualidade e integridade dos dados utilizados.  

### Perguntas que se deseja responder

1. Como evolu√≠ram os casos e mortes por COVID-19 ao longo do tempo em diferentes pa√≠ses?  
2. Qual o impacto do avan√ßo da vacina√ß√£o na redu√ß√£o de casos e mortes por pa√≠s ou continente?  
3. Quais pa√≠ses apresentaram a maior velocidade de vacina√ß√£o e como isso refletiu nos indicadores da pandemia?  
4. Existe uma defasagem temporal m√©dia entre o in√≠cio da vacina√ß√£o e a queda nos n√∫meros de mortes?  
5. Quais pa√≠ses apresentaram comportamento fora da m√©dia (casos/mortes altos mesmo com vacina√ß√£o avan√ßada)?  
6. H√° pa√≠ses onde os dados de vacina√ß√£o n√£o coincidem com a queda de casos, indicando outros fatores influenciando?  

---

## üß∞ Plataforma

A plataforma escolhida para o desenvolvimento do pipeline foi o **Databricks Community Edition**, por oferecer recursos de processamento distribu√≠do com **Apache Spark** e suporte √† arquitetura **Delta Lake** (Bronze, Silver, Gold).  

Apesar das limita√ß√µes da edi√ß√£o gratuita, foi poss√≠vel realizar toda a transforma√ß√£o, integra√ß√£o e an√°lise diretamente na nuvem, com persist√™ncia de dados e uso de notebooks Python.

---

## üîç Detalhamento

### 1. Busca pelos dados

As bases de dados foram escolhidas com base na possibilidade de responder √†s perguntas anal√≠ticas propostas. Os dados s√£o p√∫blicos e foram obtidos diretamente da plataforma **Kaggle**:

- [COVID-19 World Vaccination Progress](https://www.kaggle.com/gpreda/covid-world-vaccination-progress)
- [Covid Cases and Deaths WorldWide](https://www.kaggle.com/datasets/sudalairajkumar/novel-corona-virus-2019-dataset)

Estas bases s√£o amplamente utilizadas para estudos sobre a pandemia e n√£o possuem restri√ß√µes de uso educacional.

---

### 2. Coleta

As bases foram baixadas manualmente da plataforma Kaggle e, em seguida, enviadas para o ambiente de arquivos do **Databricks (DBFS)**. As fontes est√£o no formato `.csv` e foram lidas diretamente com PySpark para cria√ß√£o de DataFrames.

---

### 3. Modelagem

Foi constru√≠do um modelo dimensional utilizando **esquema estrela**, com **tabelas fato e dimens√µes**.  

As tabelas principais est√£o na camada **Gold**, resultantes de transforma√ß√µes nas camadas Bronze e Silver.  

#### Cat√°logo de Dados

##### üü® `vacinas_total_por_pais_fabricante`
| Coluna       | Tipo   | Descri√ß√£o                                |
|--------------|--------|------------------------------------------|
| pais         | string | Nome do pa√≠s                             |
| fabricante   | string | Nome do fabricante da vacina             |
| total_vacinas| long   | Total de vacinas aplicadas               |

##### üü® `casos_mortes_por_pais`
| Coluna       | Tipo   | Descri√ß√£o                                |
|--------------|--------|------------------------------------------|
| pais         | string | Nome do pa√≠s                             |
| total_casos  | long   | Total de casos registrados               |
| total_mortes | long   | Total de mortes registradas              |
| population   | long   | Popula√ß√£o estimada do pa√≠s               |

##### üü® `vacinas_casos_mortes_flat`
| Coluna       | Tipo   | Descri√ß√£o                                |
|--------------|--------|------------------------------------------|
| pais         | string | Nome do pa√≠s                             |
| fabricante   | string | Nome do fabricante da vacina             |
| total_vacinas| long   | Total de vacinas aplicadas               |
| total_casos  | long   | Total de casos registrados               |
| total_mortes | long   | Total de mortes registradas              |
| population   | long   | Popula√ß√£o estimada do pa√≠s               |

---

### 4. Carga

A carga foi realizada nas seguintes camadas do Delta Lake:

- **Bronze**: ingest√£o dos arquivos `.csv` originais.  
- **Silver**: limpeza e padroniza√ß√£o dos dados (casting de tipos, remo√ß√£o de nulos, renomea√ß√µes).  
- **Gold**: agrega√ß√µes, jun√ß√µes e cria√ß√£o de tabelas anal√≠ticas conforme o modelo estrela.

Todos os dados foram persistidos como tabelas Delta e podem ser reutilizados em notebooks SQL e Python no Databricks.

---

## üìí Notebook Principal

O notebook com todo o pipeline (extra√ß√£o, transforma√ß√£o, carga e an√°lise) est√° dispon√≠vel na pasta:  
üìÅ `notebook/`

---

## üìÅ Organiza√ß√£o do Reposit√≥rio


---

## üôã‚Äç‚ôÄÔ∏è Autoavalia√ß√£o

Durante a execu√ß√£o do projeto, o objetivo principal foi construir um pipeline de engenharia de dados funcional, aplicar conceitos de transforma√ß√£o com modelo estrela e criar an√°lises de valor com base em dados reais. Com base no trabalho conclu√≠do, acredito que os objetivos foram atingidos.
Entre as principais dificuldades enfrentadas, destacam-se:
‚Ä¢	Tratamento de dados inconsistentes: as bases exigiram m√∫ltiplas limpezas e convers√µes, especialmente para garantir integridade nas colunas num√©ricas.
‚Ä¢	Problemas com schemas e parti√ß√µes: alguns erros no Databricks, como conflitos de schema, exigiram ajustes de configura√ß√£o e uso de mergeSchema.
‚Ä¢	Uni√£o de tabelas complexas: por envolver jun√ß√µes de fontes distintas, foi necess√°rio alinhar nome de colunas e formatos para garantir que as queries funcionassem.
Apesar das dificuldades, o aprendizado t√©cnico foi enriquecedor, especialmente no uso do PySpark e do Databricks como ferramenta de processamento em larga escala.

## Trabalhos Futuros
Como sugest√µes para evolu√ß√£o do projeto:
‚Ä¢	Adicionar a dimens√£o temporal ao modelo, para an√°lise de tend√™ncias mais apuradas.
‚Ä¢	Explorar visualiza√ß√µes mais avan√ßadas, com dashboards interativos em Power BI ou Plotly.
‚Ä¢	Incorporar novas vari√°veis, como lockdowns, taxa de testagem ou mobilidade populacional.
‚Ä¢	Utilizar machine learning para prever evolu√ß√£o de casos com base no hist√≥rico por pa√≠s.


- **Tratamento de dados inconsistentes**: exigiu limpezas e convers√µes, especialmente em colunas num√©ricas. 
